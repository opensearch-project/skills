From c6ef0bc02a2f20127f0d7bbccfab56f8397130a2 Mon Sep 17 00:00:00 2001
From: Hailong Cui <ihailong@amazon.com>
Date: Fri, 17 Jan 2025 15:56:38 +0800
Subject: [PATCH] jar hell

Signed-off-by: Hailong Cui <ihailong@amazon.com>
---
 build.gradle | 87 +++++++++++++++++++++++++++++++++++++++++++++++++---
 1 file changed, 83 insertions(+), 4 deletions(-)

diff --git a/build.gradle b/build.gradle
index bdabba2..7387626 100644
--- a/build.gradle
+++ b/build.gradle
@@ -84,9 +84,13 @@ java {
 def sqlJarDirectory = "$buildDir/dependencies/opensearch-sql-plugin"
 def jsJarDirectory = "$buildDir/dependencies/opensearch-job-scheduler"
 def adJarDirectory = "$buildDir/dependencies/opensearch-time-series-analytics"
+def sparkDir = "$buildDir/dependencies/spark"
 
 configurations {
     zipArchive
+    spark {
+        transitive = false
+    }
     secureIntegTestPluginArchive
     all {
         resolutionStrategy {
@@ -114,6 +118,12 @@ task addJarsToClasspath(type: Copy) {
         include "opensearch-time-series-analytics-${opensearch_build}.jar"
     }
     into("$buildDir/classes")
+
+    // spark jar
+    from(fileTree(dir: sparkDir)) {
+        include("*.jar")
+    }
+    into("$buildDir/classes")
 }
 
 /*
@@ -158,9 +168,10 @@ dependencies {
     compileOnly("com.fasterxml.jackson.core:jackson-annotations:${versions.jackson}")
     compileOnly("com.fasterxml.jackson.core:jackson-databind:${versions.jackson_databind}")
 
-    implementation 'org.apache.spark:spark-sql-api_2.13:3.5.4'
-    implementation 'org.apache.spark:spark-core_2.13:3.5.4'
-    implementation group: 'org.apache.spark', name: 'spark-common-utils_2.13', version: '3.5.4'
+    spark 'org.apache.spark:spark-sql-api_2.13:3.5.4'
+    spark 'org.apache.spark:spark-core_2.13:3.5.4'
+    spark group: 'org.apache.spark', name: 'spark-common-utils_2.13', version: '3.5.4'
+
 
     //compileOnly group: 'org.apache.spark', name: 'spark-streaming_2.13', version: '3.5.4'
 
@@ -176,6 +187,7 @@ dependencies {
     implementation fileTree(dir: jsJarDirectory, include: ["opensearch-job-scheduler-${opensearch_build}.jar"])
     implementation fileTree(dir: adJarDirectory, include: ["opensearch-anomaly-detection-${opensearch_build}.jar"])
     implementation fileTree(dir: sqlJarDirectory, include: ["opensearch-sql-${opensearch_build}.jar", "ppl-${opensearch_build}.jar", "protocol-${opensearch_build}.jar"])
+    implementation fileTree(dir: sparkDir, include: ["spark*.jar"])
     compileOnly "org.opensearch:common-utils:${opensearch_build}"
     compileOnly "org.jetbrains.kotlin:kotlin-stdlib:${kotlin_version}"
     compileOnly "org.opensearch:opensearch-job-scheduler-spi:${opensearch_build}"
@@ -206,6 +218,73 @@ dependencies {
     testImplementation "commons-validator:commons-validator:1.8.0"
     testRuntimeOnly 'org.junit.jupiter:junit-jupiter-engine:5.11.2'
 }
+task addSparkJar(type: Copy) {
+    mustRunAfter()
+    from(configurations.spark)
+    into sparkDir
+
+    doLast {
+        // 3. Let's say you have A.jar and B.jar at the root of that unzipped folder:
+        def jarA = file("$sparkDir/spark-sql-api_2.13-3.5.4.jar")
+        def jarB = file("$sparkDir/spark-core_2.13-3.5.4.jar")
+        def jarC = file("$sparkDir/spark-common-utils_2.13-3.5.4.jar")
+
+        // 3a. Extract jar A to manipulate it
+        def jarAContents = file("$buildDir/tmp/JarAContents")
+        delete(jarAContents)
+        jarAContents.mkdirs()
+        copy {
+            from zipTree(jarA)
+            into jarAContents
+        }
+        // Remove the unwanted directory from jar A
+        delete file("${jarAContents}/org/apache/spark/unused")
+
+        // Re-compress jar A
+        ant.zip(destfile: jarA, baseDir: jarAContents)
+
+        // 3b. Repeat for jar B
+        def jarBContents = file("$buildDir/tmp/JarBContents")
+        delete(jarBContents)
+        jarBContents.mkdirs()
+        copy {
+            from zipTree(jarB)
+            into jarBContents
+        }
+        // Remove the unwanted directory from jar B
+        delete file("${jarBContents}/org/apache/spark/unused")
+        //delete file("${jarBContents}/org/apache/spark/SparkDriverExecutionException.class")
+        //delete file("${jarBContents}/org/apache/spark/SparkException.class")
+        //delete file("${jarBContents}/org/apache/spark/SparkUserAppException.class")
+        //delete file("${jarBContents}/org/apache/spark/SparkUserAppException\$.class")
+        //delete file("${jarBContents}/org/apache/spark/SparkUserAppException*class")
+
+
+        // Re-compress jar B
+        ant.zip(destfile: jarB, baseDir: jarBContents)
+
+        def jarCContents = file("$buildDir/tmp/JarBContents")
+        delete(jarCContents)
+        jarCContents.mkdirs()
+        copy {
+            from zipTree(jarC)
+            into jarCContents
+        }
+        // Remove the unwanted directory from jar B
+        delete file("${jarCContents}/org/apache/spark/unused")
+        delete file("${jarCContents}/org/apache/spark/SparkDriverExecutionException.class")
+        delete file("${jarCContents}/org/apache/spark/SparkException.class")
+        delete file("${jarCContents}/org/apache/spark/SparkUserAppException.class")
+        delete file("${jarCContents}/org/apache/spark/SparkUserAppException\$.class")
+        delete file("${jarCContents}/org/apache/spark/SparkUserAppException*class")
+        //delete file("${jarCContents}/org/apache/spark/java/function/MapGroupsFunction*class")
+
+
+        // Re-compress jar C
+        ant.zip(destfile: jarC, baseDir: jarCContents)
+    }
+}
+
 
 task extractSqlJar(type: Copy) {
     mustRunAfter()
@@ -228,6 +307,7 @@ task extractAdJar(type: Copy) {
 tasks.addJarsToClasspath.dependsOn(extractSqlJar)
 tasks.addJarsToClasspath.dependsOn(extractJsJar)
 tasks.addJarsToClasspath.dependsOn(extractAdJar)
+tasks.addJarsToClasspath.dependsOn(addSparkJar)
 project.tasks.delombok.dependsOn(addJarsToClasspath)
 tasks.publishNebulaPublicationToMavenLocal.dependsOn ':generatePomFileForPluginZipPublication'
 tasks.validateNebulaPom.dependsOn ':generatePomFileForPluginZipPublication'
@@ -540,7 +620,6 @@ integTest {
         jvmArgs '-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=*:5005'
     }
 }
-
 // Set up integration test clusters, installs all zipArchive dependencies and this plugin
 testClusters.integTest {
     testDistribution = "ARCHIVE"
-- 
2.39.5 (Apple Git-154)

